{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython candies...\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "feb13fc88688cd77d0f4266f0d95f6b5e341bfa4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20d07e77e30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b6e71245b5e82a99ce01f914e6efc37e5dd771b0"
   },
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "e66bf66fae3734f95241eebc5ac8d11e61718bfe"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io #codecs\n",
    "\n",
    "\n",
    "# Text version of 1000sms \n",
    "if os.path.isfile('train_singlish.txt'):\n",
    "    with io.open('train_singlish.txt'\n",
    "                 , encoding='utf8'\n",
    "                ) as fin:\n",
    "        text = fin.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "10e3d0a9d7d39774d41e60326ecf82f939dedcf6"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
    "                  for sent in sent_tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d971921fc3975505d0db84eca326744fb98c1ced"
   },
   "outputs": [],
   "source": [
    "class SinglishDataset(nn.Module):\n",
    "    def __init__(self, texts):\n",
    "        self.texts = texts\n",
    "        \n",
    "        # Initialize the vocab \n",
    "        special_tokens = {'<pad>': 0, '<unk>':1, '<s>':2, '</s>':3}\n",
    "        self.vocab = Dictionary(texts)\n",
    "        self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        \n",
    "        # Keep track of the vocab size.\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "        # Keep track of how many data points.\n",
    "        self._len = len(texts)\n",
    "        \n",
    "        # Find the longest text in the data.\n",
    "        self.max_len = max(len(txt) for txt in texts) \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        vectorized_sent = self.vectorize(self.texts[index])\n",
    "        x_len = len(vectorized_sent)\n",
    "        # To pad the sentence:\n",
    "        # Pad left = 0; Pad right = max_len - len of sent.\n",
    "        pad_dim = (0, self.max_len - len(vectorized_sent))\n",
    "        vectorized_sent = F.pad(vectorized_sent, pad_dim, 'constant')\n",
    "        return {'x':vectorized_sent[:-1], \n",
    "                'y':vectorized_sent[1:], \n",
    "                'x_len':x_len}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens, start_idx=2, end_idx=3):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        # Lets just cast list of indices into torch tensors directly =)\n",
    "        \n",
    "        vectorized_sent = [start_idx] + self.vocab.doc2idx(tokens) + [end_idx]\n",
    "        return torch.tensor(vectorized_sent)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "4e39ea3edc9855cbc71e0b4e8c18dd1ca84e827f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33295"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singlish_data = SinglishDataset(tokenized_text)\n",
    "len(singlish_data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "149f189249edf73258f9456e86c2116cc2da3150"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': tensor([[    2,   293,  1075,  ...,     0,     0,     0],\n",
      "        [    2,   817, 26297,  ...,     0,     0,     0],\n",
      "        [    2,   347,  8502,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [    2,   231,  2206,  ...,     0,     0,     0],\n",
      "        [    2,   160,   179,  ...,     0,     0,     0],\n",
      "        [    2,   145,    41,  ...,     0,     0,     0]]), 'y': tensor([[  293,  1075,   295,  ...,     0,     0,     0],\n",
      "        [  817, 26297,  8594,  ...,     0,     0,     0],\n",
      "        [  347,  8502,  2606,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  231,  2206,   115,  ...,     0,     0,     0],\n",
      "        [  160,   179,    61,  ...,     0,     0,     0],\n",
      "        [  145,    41, 33291,  ...,     0,     0,     0]]), 'x_len': tensor([50, 45, 33, 14, 12, 11, 10, 10, 10,  5])}\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "dataloader = DataLoader(dataset=singlish_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for data_dict in dataloader:\n",
    "    # Sort indices of data in batch by lengths.\n",
    "    sorted_indices = np.array(data_dict['x_len']).argsort()[::-1].tolist()\n",
    "    data_batch = {name:_tensor[sorted_indices]\n",
    "                  for name, _tensor in data_dict.items()}\n",
    "    print(data_batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5aa5c2bd3bd13d6870441dec284d7762d6b8f1bd"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, num_layers):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Initialize the embedding layer with the \n",
    "        # - size of input (i.e. no. of words in input vocab)\n",
    "        # - no. of hidden nodes in the embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size, padding_idx=0)\n",
    "        \n",
    "        # Initialize the GRU with the \n",
    "        # - size of the input (i.e. embedding layer)\n",
    "        # - size of the hidden layer \n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Initialize the \"classifier\" layer to map the RNN outputs\n",
    "        # to the vocabulary. Remember we need to -1 because the \n",
    "        # vectorized sentence we left out one token for both x and y:\n",
    "        # - size of hidden_size of the GRU output.\n",
    "        # - size of vocabulary\n",
    "        self.classifier = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs, use_softmax=False, hidden=None):\n",
    "        # Look up for the embeddings for the input word indices.\n",
    "        embedded = self.embedding(inputs)\n",
    "        # Put the embedded inputs into the GRU.\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        \n",
    "        # Matrix manipulation magic.\n",
    "        batch_size, sequence_len, hidden_size = output.shape\n",
    "        # Technically, linear layer takes a 2-D matrix as input, so more manipulation...\n",
    "        output = output.contiguous().view(batch_size * sequence_len, hidden_size)\n",
    "        # Apply dropout.\n",
    "        output = F.dropout(output, 0.5)\n",
    "        # Put it through the classifier\n",
    "        # And reshape it to [batch_size x sequence_len x vocab_size]\n",
    "        output = self.classifier(output).view(batch_size, sequence_len, -1)\n",
    "        \n",
    "        return (F.softmax(output,dim=2), hidden) if use_softmax else (output, hidden)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "dce2418b4cede1f5dd3f104179fa69defaeceac3"
   },
   "outputs": [],
   "source": [
    "# Set the hidden_size of the GRU \n",
    "embed_size = 12\n",
    "hidden_size = 10\n",
    "num_layers = 1\n",
    "\n",
    "_encoder = Generator(len(singlish_data.vocab), embed_size, hidden_size, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0d24dcec5ce293c82663ecf9ce07528d1f125882"
   },
   "outputs": [],
   "source": [
    "# Take a batch.\n",
    "batch_size = 15\n",
    "dataloader = DataLoader(dataset=singlish_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "_batch = next(iter(dataloader))\n",
    "_inputs, _lengths = _batch['x'], _batch['x_len']\n",
    "_targets = _batch['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "009c1cc8d9f7517187bd9259473703baf5cbd349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output sizes:\t torch.Size([15, 191, 33295])\n",
      "Input sizes:\t 15 191 33295\n",
      "Target sizes:\t torch.Size([15, 191])\n"
     ]
    }
   ],
   "source": [
    "_output, _hidden = _encoder(_inputs)\n",
    "print('Output sizes:\\t', _output.shape)\n",
    "print('Input sizes:\\t', batch_size, singlish_data.max_len -1, len(singlish_data.vocab))\n",
    "print('Target sizes:\\t', _targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "b5ddd1dfa0736a64152d1b3acbedb118606d4a38"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 191, 33295])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "ee773d9ff037f57b0ba49a0594f5ffb546da3498"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([191, 33295])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_output[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "07a81647f4724bc89e31eb0aaca301839d23b9fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 33295])\n"
     ]
    }
   ],
   "source": [
    "_, predicted_indices = torch.max(_output, dim=1)\n",
    "print(predicted_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "cc11b8af72aa329ceb43d3b9e40d96efea23cb84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hyperparams(embed_size=250, hidden_size=250, num_layers=1, loss_func=<class 'torch.nn.modules.loss.CrossEntropyLoss'>, learning_rate=0.03, optimizer=<class 'torch.optim.adam.Adam'>, batch_size=245)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "_hyper = ['embed_size', 'hidden_size', 'num_layers',\n",
    "          'loss_func', 'learning_rate', 'optimizer', 'batch_size']\n",
    "Hyperparams = namedtuple('Hyperparams', _hyper)\n",
    "\n",
    "\n",
    "hyperparams = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n",
    "                          loss_func=nn.CrossEntropyLoss,\n",
    "                          learning_rate=0.03, optimizer=optim.Adam, batch_size=245)\n",
    "\n",
    "hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "2654e746605c6480d1cc4cc1cf8b6c59fa3eefa4"
   },
   "outputs": [],
   "source": [
    "# Training routine.\n",
    "def train(num_epochs, dataloader, model, criterion, optimizer):\n",
    "    losses = []\n",
    "    plt.ion()\n",
    "    for _e in range(num_epochs):\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Zero gradient.\n",
    "            optimizer.zero_grad()\n",
    "            x = batch['x'].to(device)\n",
    "            x_len = batch['x_len'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            # Feed forward. \n",
    "            output, hidden = model(x, use_softmax=False)\n",
    "            # Compute loss:\n",
    "            # Shape of the `output` is [batch_size x sequence_len x vocab_size]\n",
    "            # Shape of `y` is [batch_size x sequence_len]\n",
    "            # CrossEntropyLoss expects `output` to be [batch_size x vocab_size x sequence_len]\n",
    "            _, prediction = torch.max(output, dim=2)\n",
    "            loss = criterion(output.permute(0, 2, 1), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.float().data)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(losses)\n",
    "        plt.pause(0.05)\n",
    "\n",
    "\n",
    "def initialize_data_model_optim_loss(hyperparams):\n",
    "    # Initialize the dataset and dataloader.\n",
    "    singlish_data = SinglishDataset(tokenized_text)\n",
    "    dataloader = DataLoader(dataset=singlish_data, \n",
    "                            batch_size=hyperparams.batch_size, \n",
    "                            shuffle=True)\n",
    "\n",
    "    # Loss function.\n",
    "    criterion = hyperparams.loss_func(ignore_index=singlish_data.vocab.token2id['<pad>'], \n",
    "                                      reduction='mean')\n",
    "\n",
    "    # Model.\n",
    "    model = Generator(len(singlish_data.vocab), hyperparams.embed_size, \n",
    "                      hyperparams.hidden_size, hyperparams.num_layers).to(device)\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = hyperparams.optimizer(model.parameters(), lr=hyperparams.learning_rate)\n",
    "    \n",
    "    return dataloader, model, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "5a5b3b2c17ba23fbb976f4a634ddd21e80e26c91"
   },
   "outputs": [],
   "source": [
    "def generate_example(model, temperature=1.0, max_len=100, hidden_state=None):\n",
    "    start_token, start_idx = '<s>', 2\n",
    "    # Start state.\n",
    "    inputs = torch.tensor(singlish_data.vocab.token2id[start_token]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "    sentence = [start_token]\n",
    "    i = 0\n",
    "    while i < max_len and sentence[-1] not in ['</s>', '<pad>']:\n",
    "        i += 1\n",
    "        \n",
    "        embedded = model.embedding(inputs)\n",
    "        output, hidden_state = model.gru(embedded, hidden_state)\n",
    "\n",
    "        batch_size, sequence_len, hidden_size = output.shape\n",
    "        output = output.contiguous().view(batch_size * sequence_len, hidden_size)    \n",
    "        output = model.classifier(output).view(batch_size, sequence_len, -1).squeeze(0)\n",
    "        #_, prediction = torch.max(F.softmax(output, dim=2), dim=2)\n",
    "        \n",
    "        word_weights = output.div(temperature).exp().cpu()\n",
    "        if len(word_weights.shape) > 1:\n",
    "            word_weights = word_weights[-1] # Pick the last word.    \n",
    "        word_idx = torch.multinomial(word_weights, 1).view(-1)\n",
    "        \n",
    "        sentence.append(singlish_data.vocab[int(word_idx)])\n",
    "        \n",
    "        inputs = tensor([singlish_data.vocab.token2id[word] for word in sentence]).unsqueeze(0).to(device)\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "de7efc9371e23d8ec2c73d647fdfc06a2805a573"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|██▎                                                                            | 6/201 [11:50<6:26:14, 118.84s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-bc1e3b2a03e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minitialize_data_model_optim_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-5ee12e2732b9>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(num_epochs, dataloader, model, criterion, optimizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;31m# Feed forward.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_softmax\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[1;31m# Compute loss:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[1;31m# Shape of the `output` is [batch_size x sequence_len x vocab_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-75d052b49fab>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, use_softmax, hidden)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;31m# Put it through the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;31m# And reshape it to [batch_size x sequence_len x vocab_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_softmax\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mG:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1368\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparams = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n",
    "                          loss_func=nn.CrossEntropyLoss,\n",
    "                          learning_rate=0.03, optimizer=optim.Adam, batch_size=250)\n",
    "\n",
    "dataloader, model, optimizer, criterion = initialize_data_model_optim_loss(hyperparams)\n",
    "\n",
    "train(20, dataloader, model, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "67548caaf4b02520b3eefcc87e5b7550ceff3e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> our do where now play 's yet pelvic then , how present ! ! </s>\n",
      "<s> but soon anyway news yo retuning could it raining which hai time u u which ar can happy , lot i liao for or be though i , then go always ? </s>\n",
      "<s> what < # > nobody ! </s>\n",
      "<s> im b does words another meh outside sentosa angry on u. u forward watch & ask me to a time.u who i got know ? </s>\n",
      "<s> leaving ? </s>\n",
      "<s> , did go for your prizegiving s file u dd hai later need dinner tired later up i can can email , if can sophie need good for be my suntec ? </s>\n",
      "<s> yup soon went hour.. cos block : p i was your went would be vanta u wo , u last who sure u going get its lta u while ... after , i did - in talk watch , my messy interested bah what love in ya manage i okay u got ? </s>\n",
      "<s> 6 np , then himself sinatra , `` who too one week ? </s>\n",
      "<s> what can is how np it about a she overshooting right man because together will be my sunday fromaccurate a , home some u u a will be already be shit frnd ? </s>\n",
      "<s> can . </s>\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    generate_example(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "8f2b7153dbe720c1fccbcbf5f8512a2bcce47689"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "torch.save(model.state_dict(), 'gru-model-1000sms.pth')\n",
    "\n",
    "hyperparams_str = Hyperparams(embed_size=250, hidden_size=250, num_layers=1,\n",
    "                          loss_func='nn.CrossEntropyLoss',\n",
    "                          learning_rate=0.03, optimizer='optim.Adam', batch_size=250)\n",
    "\n",
    "with open('gru-model-1000sms.json', 'w') as fout:\n",
    "    json.dump(dict(hyperparams_str._asdict()), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "fab8a4a413865aec5d69910460576fcf450896e2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "a3abe01633539dba8750208c84caecf40ca0a463"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
